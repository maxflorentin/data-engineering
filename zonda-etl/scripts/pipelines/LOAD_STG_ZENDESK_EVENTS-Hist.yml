name: LOAD_STG_ZENDESK_EVENTS-Hist
description: 'Load data from Zendesk'
owner: BI-Corp
active: true
schedule_interval: '0 5 * * *'
start_date: '2020-11-00'
catchup: true
retries: 3
retry_delay: 30
include_dummy_task: true
max_active_runs: 1
input:
  - name: date_from
    description: 'Date From in format YYYY-MM-DD'
  - name: date_to
    description: 'Date To in format YYYY-MM-DD'
  - name: shift
    description: 'Shift in days'
  - name: mode
    description: 'Partition level'
    default: days
tasks:

  - name: STAGING_Zendesk_ticket_events_Santander_Ar_Parquetize
    operator: SparkSubmitOperator
    config:
      name: STAGING_Zendesk_ticket_events_Santander_Ar_Parquetize
      connection_id: cloudera_spark2
      executor_cores: 2
      executor_memory: 8G
      num_executors: 1
      application: '$ZONDA_DIR/repositories/guyra/src/guyra.py'
      files: ['$ZONDA_DIR/repositories/zonda-etl/scripts/layers/staging/zendesk/fact/tickets_events/santander_ar/tickets_events_schema.json']
      py_files: '$ZONDA_DIR/repositories/guyra/src/config_file.py'
      conf: {
        'spark.yarn.appMasterEnv.partition_date_filter': "{{ ti.xcom_pull(task_ids='InputConfig', key='partition_date', dag_id='LOAD_STG_ZENDESK_EVENTS-Hist') }}",
        'spark.yarn.appMasterEnv.file_date_filter': "{{ ti.xcom_pull(task_ids='InputConfig', key='partition_date_filter', dag_id='LOAD_STG_ZENDESK_EVENTS-Hist') }}",
        'spark.sql.sources.partitionOverwriteMode': 'dynamic'
      }
      application_args: ['tickets_events_schema.json']

  - name: STAGING_Zendesk_ticket_envents_Santander_AddPartition
    operator: ZondaHiveOperator
    config:
      connection_id: cloudera_hive_beeline
      schema: default
      is_hql_file: true
      hql:
        - '$ZONDA_DIR/repositories/zonda-etl/scripts/layers/staging/zendesk/fact/tickets_events/santander_ar/DDLT-Create_Staging_Table.hql'
        - '$ZONDA_DIR/repositories/zonda-etl/scripts/layers/staging/zendesk/fact/tickets_events/santander_ar/ETLV-Alter_Partitions.hql'

  - name: STAGING_Zendesk_ticket_events_Comex_Parquetize
    operator: SparkSubmitOperator
    config:
      name: STAGING_Zendesk_ticket_events_Comex_Parquetize
      connection_id: cloudera_spark2
      executor_cores: 2
      executor_memory: 8G
      num_executors: 1
      application: '$ZONDA_DIR/repositories/guyra/src/guyra.py'
      files: ['$ZONDA_DIR/repositories/zonda-etl/scripts/layers/staging/zendesk/fact/tickets_events/comex_santander/tickets_events_schema.json']
      py_files: '$ZONDA_DIR/repositories/guyra/src/config_file.py'
      conf: {
        'spark.yarn.appMasterEnv.partition_date_filter': "{{ ti.xcom_pull(task_ids='InputConfig', key='partition_date', dag_id='LOAD_STG_ZENDESK_EVENTS-Hist') }}",
        'spark.yarn.appMasterEnv.file_date_filter': "{{ ti.xcom_pull(task_ids='InputConfig', key='partition_date_filter', dag_id='LOAD_STG_ZENDESK_EVENTS-Hist') }}",
        'spark.sql.sources.partitionOverwriteMode': 'dynamic'
      }
      application_args: ['tickets_events_schema.json']

  - name: STAGING_Zendesk_ticket_envents_Comex_AddPartition
    operator: ZondaHiveOperator
    config:
      connection_id: cloudera_hive_beeline
      schema: default
      is_hql_file: true
      hql:
        - '$ZONDA_DIR/repositories/zonda-etl/scripts/layers/staging/zendesk/fact/tickets_events/comex_santander/DDLT-Create_Staging_Table.hql'
        - '$ZONDA_DIR/repositories/zonda-etl/scripts/layers/staging/zendesk/fact/tickets_events/comex_santander/ETLV-Alter_Partitions.hql'

  - name: STAGING_Zendesk_ticket_events_PUC_Parquetize
    operator: SparkSubmitOperator
    config:
      name: STAGING_Zendesk_ticket_events_PUC_Parquetize
      connection_id: cloudera_spark2
      executor_cores: 2
      executor_memory: 8G
      num_executors: 1
      application: '$ZONDA_DIR/repositories/guyra/src/guyra.py'
      files: ['$ZONDA_DIR/repositories/zonda-etl/scripts/layers/staging/zendesk/fact/tickets_events/puc_santander/tickets_events_schema.json']
      py_files: '$ZONDA_DIR/repositories/guyra/src/config_file.py'
      conf: {
        'spark.yarn.appMasterEnv.partition_date_filter': "{{ ti.xcom_pull(task_ids='InputConfig', key='partition_date', dag_id='LOAD_STG_ZENDESK_EVENTS-Hist') }}",
        'spark.yarn.appMasterEnv.file_date_filter': "{{ ti.xcom_pull(task_ids='InputConfig', key='partition_date_filter', dag_id='LOAD_STG_ZENDESK_EVENTS-Hist') }}",
        'spark.sql.sources.partitionOverwriteMode': 'dynamic'
      }
      application_args: ['tickets_events_schema.json']

  - name: STAGING_Zendesk_ticket_envents_PUC_AddPartition
    operator: ZondaHiveOperator
    config:
      connection_id: cloudera_hive_beeline
      schema: default
      is_hql_file: true
      hql:
        - '$ZONDA_DIR/repositories/zonda-etl/scripts/layers/staging/zendesk/fact/tickets_events/puc_santander/DDLT-Create_Staging_Table.hql'
        - '$ZONDA_DIR/repositories/zonda-etl/scripts/layers/staging/zendesk/fact/tickets_events/puc_santander/ETLV-Alter_Partitions.hql'

dependencies:

  STAGING_Zendesk_ticket_events_Santander_Ar_Parquetize: STAGING_Zendesk_ticket_envents_Santander_AddPartition
  STAGING_Zendesk_ticket_events_Comex_Parquetize: STAGING_Zendesk_ticket_envents_Comex_AddPartition
  STAGING_Zendesk_ticket_events_PUC_Parquetize: STAGING_Zendesk_ticket_envents_PUC_AddPartition

notifications:
  on_error: true
  channels: [$DEFAULT_SLACK_CHANNEL]
  users: [mcaamano@santandertecnologia.com.ar]
name: LOAD_STG_TRIAD-RR-Weekly
description: 'Load data from TRIAD RR tables in STG (Collections Processes)'
owner: BI-Corp
active: true
start_date: '2021-02-01'
# catchup: true
retries: 0
retry_delay: 30
include_dummy_task: true
max_active_runs: 1
loop:
  values: [trfrrcc, trfrrcl, trfrrcr, trfrrct, trfrrgo, trfrrkt, trfrrvt, trfrrwt]
  iterator: e
input:
  - name: date_from
    description: 'Date From in format YYYY-MM-DD'
  - name: date_to
    description: 'Date To in format YYYY-MM-DD'
  - name: shift
    description: 'Shift in days'
    default: 0
  - name: mode
    description: 'Partition level'
    default: days
tasks:
  - name: STAGING_TABLE_Parquetize
    operator: SparkSubmitOperator
    config:
      name: 'STAGING_${e}_Parquetize'
      connection_id: cloudera_spark2
      executor_cores: 5
      executor_memory: 8G
      num_executors: 10
      jars: '$ZONDA_DIR/repositories/guyra/jars/spark-cobol-0.5.6.jar,$ZONDA_DIR/repositories/guyra/jars/cobol-parser-0.5.6.jar,$ZONDA_DIR/repositories/guyra/jars/scodec-core_2.11-1.10.3.jar'
      application: '$ZONDA_DIR/repositories/guyra/src/guyra.py'
      files: ['$ZONDA_DIR/repositories/zonda-etl/scripts/layers/staging/triad/fact/weekly/${e}/${e}_schema.json,$ZONDA_DIR/repositories/zonda-etl/scripts/layers/staging/triad/fact/${e}/${e}.cob']
      py_files: '$ZONDA_DIR/repositories/guyra/src/config_file.py'
      conf: {
        'spark.yarn.appMasterEnv.partition_date_filter': "{{ ti.xcom_pull(task_ids='InputConfig', key='partition_date', dag_id='LOAD_STG_TRIAD-RR-Weekly') }}",
        'spark.sql.sources.partitionOverwriteMode': 'dynamic'
      }
      application_args: ['${e}_schema.json']
notifications:
  on_error: true
  channels: [$DEFAULT_SLACK_CHANNEL]
  users: [nbucardo@santandertecnologia.com.ar]
name: CORE_TriggerZondaJobs
description: 'Move inbound files to HDFS and trigger depending DAGs'
owner: BI-Corp
active: true
start_date: "2019-10-24"
schedule_interval: '*/5 * * * *'
retries: 0
include_dummy_task: true
max_active_runs: 1
tasks:
  - name: WaitForFiles
    operator: ZondaFileSensor
    config:
      path: "/aplicaciones/bi/zonda/inbound"
      timeout: 300
      poke_interval: 60
      change_owner: '$SERVICE_USER:grpengineerbi'
      soft_fail: True
  - name: UploadFilesHDFS
    operator: ZondaHDFSOperator
    config:
      operation: PUT
      local_path: "{{ ti.xcom_pull(task_ids='WaitForFiles', key='return_value', dag_id='CORE_TriggerZondaJobs') }}"
      prefix: "$ZONDA_DIR/inbound"
      replace_prefix: "/santander/bi-corp/landing"
      skip_errors: true
      partitioned_by_date: true
      remove_local_path: true
  - name: TriggerDependencies
    operator: PythonOperator
    config:
      function_name: 'trigger_dag'
      function_def: |
        def trigger_dag(*args, **kwargs):
            from airflow.operators import ZondaTriggerDagRunOperator
            from itertools import groupby
            from pyzonda.hadoop import HDFS
            import requests
            import json
            import ast
            import re
            import os

            def read_config(json_file):
                with open(json_file, 'r') as f:
                    data = json.load(f)
                return data

            def regFunction(lista, regex):
                print("Mi regex: {}".format(lista[0]['path']))
                fileRegex = re.compile(regex.replace(r"\\", '\\'))
                output = [fileRegex.findall(i['path']) for i in lista]
                if output.__contains__([]):
                    output = []
                return output

            # HDFS landing path
            landing_path_hdfs = '/santander/bi-corp/landing'

            # config directory
            config_dir = os.path.dirname(os.path.realpath(__file__)).replace(os.getenv('AIRFLOW__CORE__DAGS_FOLDER') + "/autogenerated/zonda-etl", os.getenv('ZONDA_DIR') + "/repositories/zonda-etl/scripts") + "/config"
            print(os.path.dirname(os.path.realpath(__file__)))

            # join files in a multiline string
            files = ast.literal_eval(kwargs.get('files', []))
            str_files = '\n'.join(files)

            #################
            # START TRIGGER #
            #################
            for file in os.listdir(config_dir):

              try:
                print(file)
                config = read_config(config_dir+'/'+file)
                tables = "|".join([x['name'] for x in config["tables"]]).lower()
                source = "|".join([x['source'] for x in config["tables"]]).lower()
                type = "|".join([x['type'] for x in config["tables"] if x.get('type')]).lower()
                file_prefix = "|".join([x.get('filePrefix', '') for x in config["tables"]] if not config.get("regex") else [config["regex"]])
                dag_id = config['trigger']['dag']

                if type:
                  regex_expression = '^(' + landing_path_hdfs + '/(?:' + source + ')/(?:' + type + ')/(?:' + tables + ')/partition_date=([12]\d{3}-(?:0[1-9]|1[0-2])-(?:0[1-9]|[12]\d|3[01]))/(?:' + file_prefix + ')(?:[12]\d{3}(?:0[1-9]|1[0-2])(?:0[1-9]|[12]\d|3[01])).*)$'
                else:
                  regex_expression = '^(' + landing_path_hdfs + '/(?:' + source + ')/(?:' + tables + ')/partition_date=([12]\d{3}-(?:0[1-9]|1[0-2])-(?:0[1-9]|[12]\d|3[01]))/(?:' + file_prefix + ')(?:[12]\d{3}(?:0[1-9]|1[0-2])(?:0[1-9]|[12]\d|3[01])).*)$'

                regex = re.compile(regex_expression, re.MULTILINE | re.IGNORECASE)
                groups = groupby(regex.findall(str_files), key=lambda e: e[1])

                for key, _ in groups:
                    conf = {'date_from': key}

                    if config.get('regex'):

                        for x in config['tables']:
                            path_to_check_regex = ["{}/{}/{}/partition_date={}".format(landing_path_hdfs,
                                                                      x['source'].lower(),
                                                                      x['name'].lower(),
                                                                      key) if not type else "{}/{}/{}/{}/partition_date={}".format(landing_path_hdfs,
                                                                                                                            x['source'].lower(),
                                                                                                                            x['type'].lower(),
                                                                                                                            x['name'].lower(),
                                                                                                                            key)]

                        for array in [HDFS.ls(a) for a in path_to_check_regex]:
                          qFiles =+ len(array)

                        trigger_condition = eval(config['trigger'].get('condition', 'all').lower())
                        if (trigger_condition([regFunction(b, config.get('regex', None)) for b in [HDFS.ls(a) for a in path_to_check_regex]]) and qFiles == config.get('qFiles', None)):
                          try:
                              t = ZondaTriggerDagRunOperator(task_id='internal_task', trigger_dag_id=dag_id, conf=conf)
                              t.execute(context=kwargs.get('context'))
                              print("DAG '{}' triggered successfully with conf {}".format(dag_id, json.dumps(conf)))
                          except Exception as ex:
                              print("ERROR - DAG '{}' with conf {} could not be triggered [{}]".format(dag_id, json.dumps(conf), str(ex)))

                    else:

                        if type:
                          # files to check
                          files_to_check = ["{}/{}/{}/{}/partition_date={}/{}{}*".format(landing_path_hdfs,
                                                                                  x['source'].lower(),
                                                                                  x['type'].lower(),
                                                                                  x['name'].lower(),
                                                                                  key,
                                                                                  x['filePrefix'],
                                                                                  key.replace('-', '')) for x in config['tables']]
                        else:

                          files_to_check = ["{}/{}/{}/partition_date={}/{}{}*".format(landing_path_hdfs,
                                                                                         x['source'].lower(),
                                                                                         x['name'].lower(),
                                                                                         key,
                                                                                         x['filePrefix'],
                                                                                         key.replace('-', '')) for x in config['tables']]

                        trigger_condition = eval(config['trigger'].get('condition', 'all').lower())
                        if trigger_condition([HDFS.exists(x) for x in files_to_check]):
                          try:
                              t = ZondaTriggerDagRunOperator(task_id='internal_task', trigger_dag_id=dag_id, conf=conf)
                              t.execute(context=kwargs.get('context'))
                              print("DAG '{}' triggered successfully with conf {}".format(dag_id, json.dumps(conf)))
                          except Exception as ex:
                              print("ERROR - DAG '{}' with conf {} could not be triggered [{}]".format(dag_id, json.dumps(conf), str(ex)))
                        else:
                              for file in files_to_check:
                                  if HDFS.exists(file) is False:
                                      print("File {} does not exists".format(file))
              except ValueError as ve:
                print("Invalid JSON File: {}".format(ve))

            return

      provide_context: true
      op_kwargs:
        files: "{{ ti.xcom_pull(task_ids='UploadFilesHDFS', key='return_value', dag_id='CORE_TriggerZondaJobs') }}"
dependencies:
  WaitForFiles: UploadFilesHDFS
  UploadFilesHDFS: TriggerDependencies
notifications:
  on_error: true
  channels: [$DEFAULT_SLACK_CHANNEL]
  users: [nbucardo@santandertecnologia.com.ar, mjourdan@santandertecnologia.com.ar, maurogonzalez@santandertecnologia.com.ar]

